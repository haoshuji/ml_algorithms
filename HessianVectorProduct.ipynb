{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Hessian-vector product\n",
    "\n",
    "Often times we need Hessian-vector products in numerical computation / optimization. A nice family of examples of using Hessian-vector products is Krylov subspace based iterative algorithms such as Conjugate Gradient, Lanczos method, and Power method. In deep learning literature, Krylov subspace is often used in the context of 2nd-order optimization. Please refer to [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf), [Krylov Subspace Descent for Deep Learning](http://www1.icsi.berkeley.edu/~vinyals/Files/vinyals_aistats12.pdf), and [Identifying and attacking the saddle point problem in\n",
    "high-dimensional non-convex optimization](https://arxiv.org/pdf/1406.2572v1.pdf) for more detail. \n",
    "\n",
    "There are three methods of computing Hessian-vector products that we can try in TensorFlow.\n",
    "\n",
    "1. Compute Hessian directly and multiply the result with a vector.\n",
    "2. Use finite differences to get Hessian and multiply the result with a vector. (used in Marten's paper)\n",
    "3. Use Pearlmutter trick. (will explain later)\n",
    "\n",
    "In this post, I will compare these methods in terms of numerical stability and computational speed. \n",
    "I'm mainly interested in how closely well the finite differences would work when the parameter space gets big / when the condition number of Hessian becomes large, compared to Pearlmutter trick. \n",
    "\n",
    "The Method 1 is just for illustrative purpose. In practice, one should not compute Hessian directly because it's too much computation. \n",
    "\n",
    "The reason why I included method 2 is just for reference. I've tried to compute Hessian-vector product once in Torch7, which doesn't have an easy way to do the method 3. For TensorFlow / Theano users, Method 3 should suffice.   \n",
    "\n",
    "I made a TensorFlow function to compute Hessians for multilayer perceptron before, so I'll reuse it to try out the method 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#from datasets import dataset_utils\n",
    "\n",
    "# Main slim library\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method1(v):\n",
    "    H_1 = getHessian()\n",
    "    return np.dot(H_1, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method2(v):\n",
    "    with tf.Graph().as_default():\n",
    "        inputs = \n",
    "        targets = \n",
    "        # Compute finite differences of gradients\n",
    "        output, end_points = build_MLP(inputs, n_hidden=10)\n",
    "        # loss = slim.losses.sum_of_squares(predictions, targets) # Slim way\n",
    "        # The total loss is the uers's loss plus any regularization losses.\n",
    "        # total_loss = slim.losses.get_total_loss() # Slim way\n",
    "        loss = tf.reduce_mean(tf.square(output - targets))\n",
    "        grad = tf.gradients(loss) # make sure that this returns a vector, not a list of gradient and something else.\n",
    "        loss_eps = tf.add(loss, eps)\n",
    "        grad_eps = tf.gradients(loss_eps) # make sure that this returns a vector, not a list of gradient and something else.\n",
    "        Hv = tf.div(grad - grad_eps, eps)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            print(sess.run(Hv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_MLP(inputs, n_hidden=5):\n",
    "    end_points = {}\n",
    "    with slim.arg_scope([slim.fully_connected], \n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_regularizer=slim.l2_regularizer(0.01)):\n",
    "        net = slim.fully_connected(inputs, n_hidden, scope=\"fc1\")\n",
    "        end_points[\"fc1\"] = net \n",
    "        output = slim.fully_connected(net, n_output, activation_fn=None, scope=\"output\")\n",
    "        end_points[\"output\"] = output\n",
    "        \n",
    "        return output, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 : Pearlmutter trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearlmutter trick is a way to compute $Hv$ in an efficient way using backpropagation. The basic idea is to take the gradient of ----\n",
    "$$\n",
    "\\nabla_{\\theta}(g^T v) = H^T v  = Hv \n",
    "$$\n",
    ", where g is a gradient of the objective funtion with respect to parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-5bc8f0f6582b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-5bc8f0f6582b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    inputs =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def method3(v):\n",
    "    with tf.Graph().as_default():\n",
    "        inputs = \n",
    "        targets = \n",
    "        output, end_points = build_MLP(inputs, n_hidden=10)\n",
    "        loss = tf.reduce_mean(tf.square(output - targets))\n",
    "        grad = tf.gradients(loss) # make sure that this returns a vector, not a list of gradient and something else.\n",
    "        Hv = tf.gradients(tf.mul(grad, v))\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            print(sess.run(Hv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition number, as a concept, captures sensitivity of a system, which has inputs and outputs. It measures how outputs will change with respect to change in the inputs. Many problems **(examples?)** can be reduced to finding $x$ s.t. $Ax = b$. In this case, $b$ is the input and $x$ is the solution we want, and $A$ represents the structure of the problem / system you are interested in. There are a number of types of condition number depending on how to measure the \"sensitivity\" of the system. Let us focus on the case where $X$ is a symmetric matrix because then the definition of condition number is simple: \n",
    "\n",
    "---\n",
    "\n",
    "Condition number of a symmetric matrix X is defined as \n",
    "$$ \n",
    "\\frac{|\\lambda_{max}|}{|\\lambda_{min}|} \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "where $\\lambda_{max / min}$ is the maximum / minimum eigenvalue of the matrix $X$. For those who are interested to know why it is so, please refer to my other blog post on [induced matrix norm](). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a simple quadratic function for our objective funtion because it's easier to control the condition number of the Hessian; we can control the condition number of a matrix through SVD by modifying the resulting eigenvalues of the diagonal matrix returned by SVD.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any quadratic function s.t. $f(x) = \\frac{1}{2} x^T A x + b^T x + c$, the Hessian of f(x) is A when A is symmetric. So we first create a random symmetric matrix like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createRandomSymmMatrix(n_dim, condition_num): \n",
    "    random_mat = np.random.random(n_dim*n_dim).reshape(n_dim, n_dim) \n",
    "    random_mat = np.dot(np.transpose(random_mat), random_mat) # For any X, X^T X is always symmetric. \n",
    "    U,S,Vt = np.linalg.svd(random_mat) # Do svd to get diagonal elements. Note that S is a vector. \n",
    "    S = np.repeat(1.0, n_dim) # Replace the diagonal elements with a vector of 1s. \n",
    "    S[-1] = 1.0/condition_num # Let the last element of S be very small. This will determine condition number. \n",
    "    return np.dot(np.dot(U, np.diag(S)), Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56449878,  0.16759862],\n",
       "       [ 0.16759862,  0.93550122]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createRandomSymmMatrix(2, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Finite differences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Pearlmutter Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method3_c(v, n_dim, condition_num):\n",
    "    with tf.Graph().as_default():\n",
    "        inputs = createInputs(n_dim, condition_num)\n",
    "        output, matrix = build_quadratic(inputs)\n",
    "        loss = tf.reduce_sum(output - 0)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grad = tf.gradients(loss, tvars)[0]      \n",
    "        Hv = tf.gradients(tf.matmul(tf.transpose(grad), v), tvars)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            print(sess.run(grad).shape)\n",
    "            print(sess.run(tf.matmul(tf.transpose(grad), v)))\n",
    "            print(sess.run(Hv))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6793354   0.16358218 -0.0996524   0.07539995  0.12301069]\n",
      " [ 0.16358218  0.91655103  0.05083616 -0.03846414 -0.06275204]\n",
      " [-0.0996524   0.05083616  0.96903119  0.02343192  0.03822783]\n",
      " [ 0.07539995 -0.03846414  0.02343192  0.98227072 -0.0289243 ]\n",
      " [ 0.12301069 -0.06275204  0.03822783 -0.0289243   0.95281166]]\n",
      "(5, 1)\n",
      "[[ 7.53296693]]\n",
      "[array([[ 0.94167582],\n",
      "       [ 1.02975319],\n",
      "       [ 0.98187469],\n",
      "       [ 1.01371414],\n",
      "       [ 1.02237384]])]\n",
      "[[ 0.94167582]\n",
      " [ 1.02975319]\n",
      " [ 0.98187469]\n",
      " [ 1.01371414]\n",
      " [ 1.02237384]]\n"
     ]
    }
   ],
   "source": [
    "n_dim = 5\n",
    "n_cond_num = 2\n",
    "vec = np.repeat(1.0, n_dim).reshape([n_dim, 1])\n",
    "matrix = method3_c(vec, n_dim, n_cond_num) \n",
    "print(np.dot(np.transpose(matrix), vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_quadratic(inputs):\n",
    "    A = inputs[\"A\"]\n",
    "    print(A)\n",
    "    b = inputs[\"b\"]\n",
    "    c = inputs[\"c\"]\n",
    "    n_dim, _ = np.shape(A)\n",
    "    \n",
    "    x = tf.Variable(np.float64(np.repeat(1.0,n_dim).reshape(n_dim,1)))\n",
    "    \n",
    "    # Construct the computational graph for quadratic function: f(x) = 1/2 * x^t A x + b^t x + c\n",
    "    output = 0.5 * tf.matmul(tf.matmul(tf.transpose(x), A), x) + tf.matmul(tf.transpose(b), x) + c\n",
    "    return output, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createInputs(n_dim, condition_num):\n",
    "    inputs = dict() \n",
    "    inputs[\"A\"] = createRandomSymmMatrix(n_dim, condition_num)\n",
    "    inputs[\"b\"] = np.random.random(n_dim).reshape(n_dim, 1)\n",
    "    inputs[\"c\"] = np.random.random(1)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000000000005"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(createMatrix(3,10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "\n",
    "Next, let's compare the result of the above function with a method using finite differences. \n",
    "\n",
    "(By the way, there is a way to compute Hessian-vector product using only gradients, which is called Pearlmutter trick in neural network literature. Please refer to https://cswhjiang.github.io/2015/10/13/Roperator/ for more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
